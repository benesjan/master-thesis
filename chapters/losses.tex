\chapter{Loss Functions}\label{ch:loss-functions}
The main objective of the research of loss functions is to design a function which leads to a model with superior
discriminative abilities.
How well the model discriminates can be measured by the compactness of clusters (intra class variance) and the
distance between them (inter class variance).
The goal is for the clusters to be as compact as possible while maximizing the distance in-between them.

This chapter focuses on the research of loss functions used in facial recognition tasks.

\section{Softmax Loss}\label{sec:softmax-loss}
Softmax~\cite{ArcFace} is the most widely used loss function in general classification tasks.
The definition of the loss is as follows:
\begin{equation}
    \label{eq:softmax}
    \mathcal{L}_S = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{W^T_{y_i} x_{i} + b_{y_i}}}
    {\sum_{j-1}^{n} e^{W^T_{j} x_{i} + b_{j}}},
\end{equation}
where $x_i \in \mathbb{R}^{d}$ denotes the feature vector of the \textit{i}-th sample belonging to the $y_i$-th class.
$W_j \in \mathbb{R}^{d}$ is the \textit{j}-th column of the weight matrix $W \in \mathbb{R}^{d \times n}$ and \textit{b}
is the corresponding bias term.
\textit{N} is the batch size and \textit{n} is the class number.

There is one major drawback of softmax loss.
It doesn't encourage cluster compactness.
In other words, it fails to guarantee similarity among samples within a category.
This makes the learned features not discriminative enough for the open-set face recognition
problem~\footnote{\label{foot:openset}Identifying samples which were not present in the training dataset.}.

Another issue is the dimension of the output weight matrix which grows linearly with the number of identities in the
training set.
This makes softmax loss impractical for a large scale deployment.

\section{Triplet Loss}\label{sec:triplet-loss}
Triplet loss~\cite{TripletLoss} is a loss function which can be optimized by minimizing the distance between
\textit{anchor} and \textit{positive} point while maximizing the distance between \textit{anchor} and \textit{negative}
point.
These points are represented by vectors in the feature space.
The learning process is illustrated in the figure~\ref{fig:tripletloss}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{images/face-recognition/tripletloss.jpeg}
    \caption{Illustration of the triplet loss~\cite{TripletLoss}}
    \label{fig:tripletloss}
\end{figure}

Anchor and positive points belong to the same class whereas negative point belongs to another one.
In the context of facial recognition one class is one identity.
This makes the anchor and the positive point a vectorised representation of two images of one face.

In mathematical terms the loss can be described using the Euclidean distance function as follows:
\begin{equation}
    \sum_{i}^{N} \left[ \norm{f(x_{i}^{a}) - f(x_{i}^{p}))}^2_2
    - \norm{f(x_{i}^{a}) - f(x_{i}^{n}))}^2_2 + \alpha \right],
\end{equation}
where $f(x_{i}^{a})$, $f(x_{i}^{p})$ and $f(x_{i}^{n})$ are the feature vectors of the anchor and the positive and
negative point.
The index \textit{i} denotes the triplet.
\textit{N} is the number of triplets in the dataset.

The drawbacks of \textit{triplet loss} are the demands entailed by the construction of the triplets.
The number of those is subject to a combinatorial explosion and inevitably results in a slow convergence and
instability.
This is a serious issue especially for large datasets.

\section{Center Loss}\label{sec:center-loss}
The main goal of this research~\cite{CenterLoss} was to design a loss function which solves the main drawbacks of
the two previously mentioned losses
(\textit{sofmax loss}~\ref{sec:softmax-loss} and \textit{triplet loss}~\ref{sec:triplet-loss}).

The discriminative power of the learned features is enhanced by incorporation of the distance of features to the
corresponding class centers.
In other words, the network is penalized whenever the features are too far from the class center.
In the course of training the the centers are updated and the distances of features from centers are minimized.
The model is trained under a joint supervision of softmax loss and center loss.
The effect of these two losses is balanced by a hyperparameter\footnote{A parameter which is not a subject to the
learning process}.
This joint supervision results in a loss function which combines the best of both worlds:
inter-class discriminative power of \textit{softmax loss} and intra-class distance minimization of \textit{center loss}.

The center loss function is intuitively defined by the following equation:
\begin{equation}
    \label{eq:center}
    \mathcal{L}_C = \frac{1}{2} \sum_{i=1}^{m} \norm{x_i - c_{y_i}}_2^2,
\end{equation}
where $c_{y_i} \in \mathbb{R}^{d}$ denotes the $y_i$-th class center of the features.
Recomputing the centers by taking the average of the features over the whole training set is too inefficient and
impractical.

To address this problem the authors proposed updating the centers based on a mini-batch instead of the whole training
set.
To avoid large perturbations caused by mislabeled samples, there is a parameter $\alpha$ with which we control the
learning rate of the centers.

The equation~\ref{eq:centup} defines the center update:
\begin{align}
    \frac{\partial \mathcal{L}_C}{\partial x_i} &= x_i - c_{y_i} \\
    \Delta c_j &= \frac{\sum_{i=1}^m \delta(y_i=j) \cdot (c_j-x_i)}{1+\sum_{i=1}^m \delta(y_i=j)}, \label{eq:centup}
\end{align}
where $\delta(condition)$ is $1$ if the condition is satisfied and $0$ otherwise.

The final loss is described by the following formula:
\begin{equation}
    \label{eq:centerloss}
    \mathcal{L} = \mathcal{L}_S + \mathcal{L}_C = -\sum_{i=1}^{m} \log \frac{e^{W^T_{y_i} x_{i} + b_{y_i}}}
    {\sum_{j-1}^{n} e^{W^T_{j} x_{i} + b_{j}}} + \frac{\lambda}{2} \sum_{i=1}^{m} \norm{x_i - c_{y_i}}_2^2,
\end{equation}

Figure~\ref{fig:centerlosslambda} is a great visualization of the effect of hyperparameter $\alpha$ upon the cluster
compactness.
As is to be expected, higher values of the parameter result in clusters which are more compact.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{images/face-recognition/centerlosslambda.png}
    \caption{Distribution of the features for different values of hyperparameter $\lambda$~\cite{CenterLoss}.
    The white dots $(c_0,c_1,\dots,c_9)$ denote 10 class centers.}
    \label{fig:centerlosslambda}
\end{figure}

\section{Congenerous Cosine Loss}\label{sec:coco-loss}
Congenerous Cosine Loss~\cite{CocoLoss} (known as COCO loss) is a research presenting a loss function which was
published in 2017.

To define the COCO loss we first have to clarify how the centroid of class \textit{k} is computed.
The centroid\footnote{Center of cluster.} equation is the following:
\begin{equation}
    \boldsymbol{c}_{k} = \frac{\sum_{i \in \beta} \delta \left( l_i, k \right)\boldsymbol{f}^{(i)}}
    {\sum_{i \in \beta} \delta \left( l_i, k \right) + \epsilon} \in \mathbb{R}^{D \times 1},
\end{equation}
where $\beta$ is a mini-batch, $D$ is the feature space dimension, $\delta$ is an indicator function, $\epsilon$ is a
trivial number for computation stability, $\boldsymbol{f}^{(i)}$ is the \textit{i}-th feature vector and $l_i$ is
its label.

Having the centroid equation defined we can proceed with a definition of the term we are trying to maximize during
a model fitting:
\begin{equation}
    p_{l_i}^{(i)} = \frac{exp C(\boldsymbol{f}^{(i)}, \boldsymbol{c}_{l_{i}})}
    {\sum_{k \neq l} exp C(\boldsymbol{f}^{(i)}, \boldsymbol{c}_{k})} \in \mathbb{R},
\end{equation}
where $C$ is standard cosine similarity.

Now we can move to the final COCO loss function definition:
\begin{equation}
    - \sum_{i \in \beta} \log \left( p_{l_i}^{(i)} \right).
\end{equation}

\section{SphereFace Loss}\label{sec:sphereface-loss}
SphereFace~\cite{SphereFace} is a loss function which was published in 2018.
The research significantly distinguishes itself from the previously mentioned losses by not relying on an euclidean
margin.
SphereFace uses angular margin instead.
This has proven to be highly effective in face recognition tasks.
The name of the loss hints how to features are transformed during the loss computation.
The features are projected onto a hypersphere manifold.

SphereFace originates from the softmax loss~\ref{sec:softmax-loss}.
To derive SphereFace from softmax we first incorporate the angle into the softmax equation using the dot
product definition ($a \cdot b = \norm{a} \norm{b} \cos \theta$):

\begin{align*}
    \mathcal{L}_S &= -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{W^T_{y_i} x_{i} + b_{y_i}}}
    {\sum_{j-1}^{n} e^{W^T_{j} x_{i} + b_{j}}} \\
    &= -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\norm{W_{y_i}} \norm{x_i} \cos(\theta_{y_i,i}) + b_{y_i}}}
    {\sum_{j-1}^{n} e^{\norm{W_j} \norm{x_{i}} \cos(\theta_{j,i}) + b_{j}}},
\end{align*}
where $\theta_{j,i}$ is the angle between vector $W_j$ and $x_i$.
The meanings of the remaining symbols are equal to those in the softmax equation~\ref{eq:softmax}.


Next we normalize $\norm{W_j} = 1, \forall j$ and set the bias term to 0.
\begin{equation}
    \label{eq:softmaxmod}
    \mathcal{L}_{modified} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\norm{x_i} \cos(\theta_{y_i,i})}}
    {\sum_{j-1}^{n} e^{\norm{x_{i}} \cos(\theta_{j,i})}}
\end{equation}
While it's possible to learn features with the modified loss the result would not be discriminative enough.
To solve this issue the researches incorporated angular margin:
\begin{equation}
    \label{eq:ang}
    \mathcal{L}_{ang} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\norm{x_i} \cos(m\theta_{y_i,i})}}
    {e^{\norm{x_i} \cos(m\theta_{y_i,i})} + \sum_{j \neq y_i} e^{\norm{x_{i}} \cos(\theta_{j,i})}},
\end{equation}
where $\theta_{y_i,i}$ lies in $\left[ 0, \frac{\pi}{m} \right]$.

The decision boundary for a binary case is defined by:
\begin{equation}
    \label{eq:spherebound}
    \cos{m\theta_1} = \cos{\theta_2},
\end{equation}
where $\theta_i$ is the angle between the feature and weight of class $i$.

To make the loss~\ref{eq:ang} optimizable for CNNs the definition range of $\cos(\theta_{y_i},i)$ is expanded.
This is achieved by replacing the cosine term with monotonically decreasing angle function $\Psi(\theta_{y_i},i)$
\begin{equation}
    \mathcal{L}_{ang} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{\norm{x_i} \Psi(m\theta_{y_i,i})}}
    {e^{\norm{x_i} \Psi(m\theta_{y_i,i})} + \sum_{j \neq y_i} e^{\norm{x_{i}} \Psi(\theta_{j,i})}}
\end{equation}
The angle function has the following definition:
\begin{equation}
    \Psi(\theta_{y_i,i}) = (-1)^{k} \cos(\theta_{y_{i},i}) -2k,
\end{equation}
where $k \in \left[ 0, m-1 \right]$.
The parameter $m \geq 1$ gives us control over the angular margin size.

\section{CosFace Loss}\label{sec:cosface}
CosFace~\cite{CosFace} is another loss using margin to improve discriminative power of softmax~\ref{sec:softmax-loss}.

The decision boundary for binary case is defined by the following equation:
\begin{equation}
    \label{eq:cosbounary}
    \cos{\theta_1} - m = \cos{\theta_2},
\end{equation}
where the symbols have exactly the same meaning as in the equation~\ref{eq:spherebound} of SphereFace decision boundary.
Since the decision boundary of CosFace is not being defined over the angular space the loss is easier to optimize than
SphereFace.
Optimization in angular space is more difficult because of the non-monotonicity of the cosine function.

Another innovation over SphereFace is the fact that not only is the weight vector $W_j$ normalized, but the feature
vectors $x_i$ as well.
This results in a much lower intra-class variability of the learned features as the emphasis is being put on the
angle during training.
By fixing $\norm{x}$ as some predetermined radius $s$ in equation~\ref{eq:softmaxmod} we get:
\begin{equation}
    \mathcal{L}_{ns} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{s \cos(\theta_{y_i,i})}}
    {\sum_{j-1}^{n} e^{s \cos(\theta_{j,i})}}
\end{equation}
This loss is called Normalized Softmax Loss (NSL) in the CosFace research paper.
NSL emphasizes correct classification but is not discriminative enough for face recognition tasks.
For this reason (as was mentioned in the SphereFace research as well) the margin is incorporated:
\begin{equation}
    \label{eq:cosface}
    \mathcal{L}_{lmc} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{s \left( \cos(\theta_{y_i,i}) - m \right)}}
    {e^{s\ \left( \cos(\theta_{y_i,i}) - m \right)} + \sum_{j \neq y_i}^n e^{s\ \cos(\theta_{j,i})}},
\end{equation}
subject to
\begin{align}
    W &= \frac{W*}{\norm{W*}}, \\
    x &= \frac{x*}{\norm{x*}}, \\
    \cos(\theta_j, i) &= W_{j}^T x_i.
\end{align}
The meaning of the equation constituents is equivalent to those in the previous sections.

The equation~\ref{eq:cosface} is the final form of the CosFace loss.
The authors of the research call it the Large Margin Cosine Loss (LMCL).

\section{ArcFace Loss}\label{sec:arcface}
ArcFace loss is used in this thesis which makes it a standalone chapter~\ref{ch:arcface}.