\chapter{Facial Recognition}\label{ch:face-rec}
Facial recognition is a task of verifying or identifying a person from digital image/video.

As I mentioned in the definition, there are two main subtasks~\cite{FaceRec}:
\begin{enumerate}
    \item \textbf{Verification} deals with verifying whether the person in the image is who he claims he is.
    A typical modern use case of verification is smartphone unlocking with face.
    An example of such system is Face ID developed by Apple Inc.

    \item \textbf{Identification} is a task of matching a person to an identity.
    To formulate it in another way, the goal of identification is to give us an answer to the question of who the person
    in the image is.
\end{enumerate}

\section{Pipeline}\label{sec:pipeline}
Facial recognition pipeline\footnote{\label{foot:pipe}A chain of processing elements, arranged so that the output of
each element is the input of the next.} usually has the following four steps:
\begin{enumerate}
    \item The first one is \textbf{face detection}.
    As the name implies, it deals with the determination of face location within the image.
    Usually the output of the algorithm is face coordinates and facial landmarks.
    The landmarks are a set of coordinates marking important points of the face (eyebrows, nose, mouth, \ldots).
    The knowledge of these points is necessary for the following step.
    An example of face detection system is described in section~\ref{sec:face-detection}.
    \item \textbf{Face alignment} is a task of changing the face position in such a way that it resembles the position
    of faces on which the feature extraction model was trained.
    In most of the instances, this step improves the accuracy.
    \item \textbf{Feature extraction} is a process of computing a feature vector\footnote{A feature vector is a vector
    that contains information describing an object's important characteristics.} from the face.
    Architectures of models used for the feature extraction were described in the previous chapter~\ref{ch:cnn}.
    \item \textbf{Feature matching} uses the feature vector from the previous task to classify a person in the image.
    The algorithm uses a database of pre-computed feature vectors and compares them to the newly extracted one.

    If we are dealing with the \textbf{closed-set problem}~\footnote{\label{foot:closedset}Identifying samples which
    were present in the training dataset.}, the identity associated with the feature vector which has
    the smallest distance from the extracted one is considered to be the identity of the person in the image.

    For the \textbf{open-set problem}~\footnote{\label{foot:openset}Identifying samples which were not present in the
    training dataset.}, the process is similar with the difference being an addition of a threshold.
    This thresholds states the maximum distance from the closest pre-computed feature vector for the newly extracted
    vector to still be classified as that identity.
    In case this condition is not met for any of the identities in the database, we establish the vector as a new
    identity.
\end{enumerate}

It is important to note that the second step is not always present and it is deemed unnecessary by some~\cite{FaceNet}.

\include{chapters/face-recognition/systems}

\include{chapters/face-recognition/face-detection}

\section{Datasets}\label{sec:datasets}
In this section, I will briefly describe datasets used for training and evaluation of facial recognition models.
There are too many different datasets used in practice.
For this reason, I will focus only on those mentioned in this text.

\subsection{LFW}\label{subsec:lfw}
LFW is an acronym for Labeled Faces in the Wild~\cite{LFW}.
The dataset contains 13,000 images and 1680 identities.
Every identity is represented by at least two samples.
The faces were detected by Viola-Jones face detector\footnote{Real-time object detection framework.}.

There are now four publicly used versions of the dataset.
These versions are differentiated by the type of preprocessing (different methods of alignment) applied to the images.

\subsection{YTF}\label{subsec:ytf}
YTF stands for YouTube Faces~\cite{YTF}.
The data set contains 3425 videos and 1,595 unique identities.
The average length of the video clip is 181.3 frames and there are on average 2.15 videos for each subject.

\subsection{MS-Celeb-1M}\label{subsec:ms1m}
MS-Celeb-1M is a dataset constructed by Microsoft Research~\cite{MSCeleb1M}.
There are 10 million face images with nearly 100,000 individuals.
The data were harvested from the Internet.

Due to the method with which the images were collected, there are many mislabellings in the dataset.
For this reason, there are different versions available on the Internet containing refined data (like MS1MV2).

As the name implies, the dataset contains images of celebrities.
In this context, celebrity is assumed to be anyone with frequent online presence.
This became a controversial issue, and as a result, Microsoft pulled the dataset off the Internet.

\subsection{CASIA WebFace}\label{subsec:casia}
CASIA WebFace~\cite{Casia} is a dataset used for scientific research of unconstrained face recognition.
There are approximately 500,000 images and more than 10,000 identities in the database.
The images were crawled~\footnote{Automatically located and downloaded from the Internet.} from the Internet
by Institute of Automation, Chinese Academy of Sciences.

\subsection{Czech News}\label{subsec:czenew}
\begin{wrapfigure}[18]{r}{0pt}
    \centering
    \raisebox{0pt}[\dimexpr\height-0.831\baselineskip\relax]{%
    \begin{forest}
        for tree={
        font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        edge path={
        \noexpand\path [draw, \forestoption{edge}]
        (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
        },
        before typesetting nodes={
        if n=1
        {insert before={[,phantom]}}
        {}
        },
        fit=band,
        before computing xy={l=15pt},
        }
        [annotations
        [name1
        [detections
        [detection1
        [bounding\_box]
        [frame\_number]
        ]
        [detection2
        [bounding\_box]
        [frame\_number]
        ]
        [\ldots]
        ]
        ]
        [name2
        [detections
        [detection1
        [bounding\_box]
        [frame\_number]
        ]
        [\ldots]
        ]
        ]
        [\ldots]
        ]
    \end{forest}
    }
    \caption{Format of annotations}
    \label{fig:annotations}
\end{wrapfigure}
Czech News is a dataset which was created from the recordings of evening news on the czech public television broadcast
(Česká Televize).
The dataset consists of 15 videos and the same amount of annotation files in \textit{json}\footnote{An
open-standard file format that uses human-readable text to transmit data objects consisting of attribute–value pairs
and array data types.} format.

In every annotation file (visualized in figure~\ref{fig:annotations}), there is a dictionary object,
where key is a name, and value is a list of detections.
Every detection contains information about the frame (frame number) and the position of the face (bounding box).
This information is later used to process the dataset (section~\ref{sec:data-preprocessing}).
The reference face position was selected by a human.

In the dataset there are 1238 identities and 732607 detections.

As I mentioned in the section about EyeFace SDK~\ref{subsec:eyeface}, this dataset is used to evaluate my
implementation of the face recognition algorithm (chapter~\ref{ch:implementation}).