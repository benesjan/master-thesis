\chapter{Convolutional Neural Networks}\label{ch:cnn}
Convolutional Neural Network (CNN) is a class of Artificial Neural Networks\footnote{A computational model inspired by
animal and human brains.}, which allows for efficient training on high dimensional data.
This is especially useful in the field of \textit{computer vision}\footnote{A scientific field dealing with extraction
of high level understanding from imagery using computers.} as image data is fundamentally high dimensional.

This thesis is dealing with a subset of computer vision called \textit{face recognition}\ref{ch:face-rec} in which CNNs
achieve state-of-the-art results.

The typical architecture of CNNs contains many layers.
Because of that, these models belong to the class of machine learning methods called
\textit{deep learning}\footnote{A set of machine learning models with \textit{credit assignment path (CAP)} higher than 2.
The CAP is the chain of transformations from input to output.}
In the following section~\ref{sec:layer-types}, I will deal with layer types used in CNNs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{images/cnn/lenet.eps}
    \caption{Example of CNN model (LeNet 5)~\cite{LeNet5}}
    \label{fig:cnn}
\end{figure}

\section{Layer Types}\label{sec:layer-types}
The image~\ref{fig:cnn} is an illustration of one of the oldest CNN models, called \textit{LeNet-5}, which was used for
handwritten character recognition.
There are three layer types in the architecture: \textit{dense}~\ref{subsec:dense},
\textit{convolutional}~\ref{subsec:convolutional}, and \textit{pooling}~\ref{subsec:pooling} layer.
Modern CNNs also usually contain \textit{locally connected layer}~\ref{subsec:lclayer}.

\subsection{Dense/Fully Connected}\label{subsec:dense}
The dense layer is the simplest type of layer present in CNN models.
Its output is determined simply by multiplication of its \textit{inputs} (\textit{x}) with \textit{weight matrix}
(denoted \textit{W} in~\ref{eq:dense}) and addition of \textit{bias} (\textit{b}):
\begin{equation}
    \label{eq:dense}
    h[i, j] = b[i,j] + \sum_{k,l} W[i,j,k,l] \cdot x[i+k,j+l],
\end{equation}
where $x\left[ i, j \right]$ and $h\left[ i, j \right]$ denote pixel location $\left( i, j \right)$ in an image and
hidden representation, respectively.
The indices $k$, $l$ run over both positive and negative offsets, covering the entire image.

To demonstrate the number of parameters needed, let's imagine that we want to feed a greyscale image which is 256 pixels
high and wide to a dense layer.
First we flatten the image, which results in a vector with $256\cdot256 = 65536$ dimensions.
Even if we do aggressive reduction to 1000 hidden dimensions, we end up with approximately 65 million parameters.
This makes the dense layer impractical when dealing with imagery.
For this reason, a convolutional layer~\ref{subsec:convolutional} is usually used as the input layer of computer
vision models.

\subsection{Convolutional}\label{subsec:convolutional}
As I hinted in the previous section, the main advantage of CNNs~\cite{ConvLayer} is the low amount of parameters needed.
In the convolutional layer, this feat was achieved by the application of two principles:
\textit{invariance}~\ref{subsec:invariance} and \textit{locality}~\ref{subsec:locality}.

\subsubsection{Invariance Principle}\label{subsec:invariance}
The core of the invariance principle is a reuse of weights.
This is achieved by the application of weights on one part of the image, shifting the weights by a predetermined set
of pixels (called a stride), and then applying the weights again.
Let us examine the equation~\ref{eq:denseinv} to see how the original equation~\ref{eq:dense} changes.

\begin{equation}
    \label{eq:denseinv}
    h[i, j] = b + \sum_{k,l} W[k,l] \cdot x[i+k,j+l]
\end{equation}

As is to be expected, bias \textit{b} and the weight matrix \textit{W} are no longer dependent upon the image
coordinates \textit{(i, j)}.
As an example, we can think of an airplane detection algorithm whose goal is to find whether there is an airplane
present in any part of the scene.
The core principle used during the algorithm design would be sliding one set of weights (kernel) describing the
airplane over the image.
The algorithm would then classify the scene with high impulse response as containing an airplane.

This type of invariance is called \textit{translational invariance}.

\subsubsection{Locality Principle}\label{subsec:locality}
Another principle used in CNNs is called \textit{locality principle}.
This principle suggests, that we do not need to look far away from \textit{(i,j)} to gain valuable information about
what is going on in that particular location.
This is achieved, mathematically speaking, by limiting \textit{k} and \textit{l} to a range $\Delta$.

\begin{equation}
    \label{eq:denseinvloc}
    h[i, j] = b + \sum_{k=-\Delta}^{\Delta} \sum_{l=-\Delta}^{\Delta} W[k,l] \cdot x[i+k,j+l]
\end{equation}

It is important to note that the range $\Delta$ is also known as the kernel size.
Equation~\ref{eq:denseinvloc} is the final form describing the convolution layer.

\subsection{Pooling}\label{subsec:pooling}
The main objective of pooling is to decrease the spatial dimension of the inner representation of the data.
During classification (very common application of CNNs) we are not interested in the location of the classified
object within the image.
The only information that interests us is whether the object is present in the scene or not.
With this information in mind, pooling~\cite{PoolingLayer} was invented.

The pooling layer is similar to the convolutional layer in a sense that both are "looking" only at a part of the input
at once.
This point of view (window) is then shifted as I described in section~\ref{subsec:invariance}.
What makes the pooling layer different from the convolutional one is that there is no kernel present in the operation.
The only thing pooling does is that it selects/computes the most representative value from the window.
There are two common pooling types: \textit{max pooling} and \textit{average pooling}.
The first type selects the maximum value and the second one computes the average.

Stacking pooling layer on top of convolutional layer is a powerful combination as pooling makes the output of CNNs
more robust to local translations\cite{DeepFace}.

\subsection{Locally Connected}\label{subsec:lclayer}
Locally connected layer is similar to convolutional layer but the difference is that every location in the feature map
learns different set of filters.
This layer type is, for example, used by the DeepFace system~\ref{subsec:deepface}.

\section{Other Concepts}\label{sec:otherconc}
This section is about concepts which are not defined as a layer but form an important piece of modern CNNs nonetheless.
To name these concept specifically, they are Batch Normalization~\ref{subsec:batchnorm} and
Rectifier~\ref{subsec:rectifier} and Padding~\ref{subsec:padding}.

\subsection{Batch Normalization}\label{subsec:batchnorm}
Batch Normalization is a method of speeding up the training process of Artificial Neural
Networks (ANNs)~\cite{BatchNormalization}.
According to the authors, this feat is achieved by a reduction of the negative impact of \textit{internal covariate
shift}.
\textit{Covariate shift} is a change in the input distribution of learning systems.
When this concept is applied not to the whole system, but only to its part, we call this phenomenon an \textit{internal
covariate shift}.

During the training the change of input distribution is caused by adjustments of the parameters of the preceding layers.
This makes the optimal parameter setting in the hidden layers a moving target which dramatically slows down the
convergence.

This issue is alleviated by an incorporation of normalization into the model architecture, i.e., by fixing the means
and variances of layer inputs.

The mean and variance is computed over the current batch which is the reason why this method is called Batch
Normalization.

However, new research has shown that the \textit{internal covariate shift} might not be the main
reason for the improved speed of convergence~\cite{BNormCorrection}.
The researches reintroduced the covariate shift into the training process by an injection of i.i.d. noise sampled
from a non-zero mean and non-unit variance distribution into the outputs of the Batch Normalization layer.
Surprisingly, the change in the speed of convergence was negligible.
According to the research paper, the profound effects of Batch Normalization are caused by the reparametrization
of the underlying optimization problem into a form, which is more stable and smooth.
These two traits result in a training process with gradients that have higher predictive power.
This enables faster and more effective optimization.

The output $y_i$ of the normalization step is computed using the following formula:
\begin{equation}
    y_i = \gamma \bar{x}_i + \beta,
\end{equation}
where $\bar{x}_i$ is the average of the current batch.
Parameters $\gamma$ and $\beta$ are adjusting scale and shift, respectively.
These parameters are learned during training.
They were incorporated into the formula in order to restore the representative power of the network.

\subsection{Padding}\label{subsec:padding}
To avoid loss of information at the edges of the image we usually use a method called \textit{padding}.
This technique increases the image dimension by pixel addition around the original image.
There are few padding variants which are differentiated by the value of the new pixels.
The most common ones are \textit{zero padding} and \textit{reflective padding}.
The first-mentioned type, as the name implies, sets the new pixels to zero.
The second one is more sophisticated and consists of mirroring of the neighboring pixels.

\subsection{Rectifier}\label{subsec:rectifier}

\begin{wrapfigure}{r}{7cm}
    \label{fig:relu}
    \includegraphics[width=7cm]{images/cnn/relu.eps}
    \caption{Illustration of ReLU activation function~\cite{ReLU}}
\end{wrapfigure}

Another important innovation of modern CNNs is rectifier activation function\footnote{The activation function of
a node defines the output of that node given an input or set of inputs.} defined as:
\begin{equation}
    \label{eq:relu}
    \mathrm{ReLU}(x) = max(0,x),
\end{equation}
where $x$ is the scalar input.

A unit implementing this activation function is called a rectifier linear unit (ReLU)~\ref{eq:relu}.
In CNNs ReLUs are usually positioned at the output of convolutional layer~\ref{subsec:convolutional}.

The advantage of ReLU is its efficacy during the training and the reduced likelihood of a vanishing
gradient\footnote{\label{foot:vangrad}A situation where a deep neural network is unable to propagate useful gradient
information from the output end of the model back to the layers near the input end of the model.}.

\include{chapters/cnn/models}